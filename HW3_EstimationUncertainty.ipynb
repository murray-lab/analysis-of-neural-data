{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework assignment we will focus on estimation and uncertainty, making use of material from Chapters 6 and 7 from Kass, Eden, and Brown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The law of large numbers and the central limit theorem\n",
    "\n",
    "Chapter 6 in the textbook introduces the law of large numbers (LLN) and the central limit theorem (CLT). Suppose that we draw some random variables $X_1, X_2, \\ldots, X_n$ from some distribution with mean $\\mu$ and compute the mean $\\bar{X}$, which we expect to be close to but not exactly equal to the true mean $\\mu$. The LLN and CLT state the following:\n",
    "\n",
    "- **LLN**: The sample mean $\\bar{X}$ approaches the true mean as $n$ increases.\n",
    "- **CLT**: The distribution of $\\bar{X}$ becomes normal as $n$ becomes large. This is true even if the distribution of $X$ isn't normal.\n",
    "\n",
    "In what follows, we will illustrate these two important statistical principles using simulated data for the particular case of binomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The binomial distribution\n",
    "\n",
    "Recall that a binomial distribution describes a sum of binary random variables, and the distribution is characterized by a single parameter $p$. Imagine that we have a (not necessarily fair) coin that comes up heads with probability $p$ and tails with probability $1-p$. We can do an \"experiment\" in which we flip the coin $n$ times and let $X_i = 1$ if it comes up heads or $X_i = 0$ if it comes up tails on the $i$th flip. We can say that $Y = \\sum_{i=1}^n X_i$ is a random variable drawn from a binomial distribution $\\mathcal{B}(n,p)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLN for the binomial distribution\n",
    "\n",
    "The true mean of the binomial distribution is $E(Y) = np$, and the LLN says that $Y$ should get closer and closer to this value as $n$ becomes larger. Our first problem will be to show that this is true by doing a numerical experiment with simulated data, like the coin flip experiment described above.\n",
    "\n",
    "To begin, we need to figure out how to flip a coin in Python. We can do this using the function `np.random.rand()`, which returns a random value between 0 and 1. If $p$ is the probability that the coin comes up heads, we can simulate a coin flip by saying that the result is heads if `np.random.rand()` $< p$, and tails otherwise. (Think this through to make sure that it makes sense!) Then, if we want to generate a binomial random variable $Y \\sim \\mathcal{B}(n,p)$, we just need to do this $n$ times and add up the number of heads flips.\n",
    "\n",
    "The code below shows an example of how this can be done. Since the probability of heads in this example is $p=0.8$ and we are doing 20 flips, we expect the number of heads, `y`, to be about 16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "p = 0.8\n",
    "n = 20\n",
    "y = 0\n",
    "for i in range(n):\n",
    "    if np.random.rand() < p:\n",
    "        y += 1\n",
    "        \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "Turn the above code into a function, `binomial(n,p)` that takes `n` and `p` as arguments and returns a binomial random variable. After writing this function, try printing the result of `binomial(20, 0.8)` and make sure that it gives a reasonable value as in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Draw 10000 samples from the binomial distribution with $m=20$ and $p=0.8$ and plot a histogram of the results. Since the random variables are integers, make sure that each bin in the histogram has a width of 1.\n",
    "\n",
    "**Note**: If you're not confident that you got the previous exercise correct, you can use the built-in function `np.random.binomial()` to generate binomial data in this and the following exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLN says that we expect $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$ to get closer and closer to its theoretical mean, $p$, as $n$ increases. For the binomial distribution, where $Y = \\sum_{i=1}^n X_i$, this is the same as saying that $Y$ approaches $np$ as $n$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "Let `n_array = np.array([10, 100, 1000, 10000, 100000])` be an array of values of `n`. Use a `for` loop to print `n` along with $Y/n$ for each value of `n`, where $Y \\sim \\mathcal{B}(n,p)$ is a binomial random variable. Use $p=0.8$ as above. Do the values get closer to the true mean as `n` gets larger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result is intuitive, but it isn't entirely satisfying since, due to randomness, it will give different values each time we run the code. What we really want to know is the *distribution* of $\\bar{X}$ for each value of  $n$, since this would give us information about the amount of variability that the sample mean exhibits about the true mean. The CLT gives us this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CLT for the binomial distribution\n",
    "\n",
    "According to the CLT, the distribution of $\\bar{X}$ should become normal as $n$ becomes large. We can see this by plotting a histogram below, in this case with $n=500$.\n",
    "\n",
    "We can notice two things:\n",
    "1. The distribution looks much more like a normal distribution (i.e. a bell-shaped curve) than the original binomial distribution that we plotted above. This is the crucial point about the CLT: If $n$ is large, then distributions of *means* are normal, even if the distribution from which the data is drawn is not normal. This is very important to keep in mind with experimental data, where the CLT means that we can still know something about the distribution of means, even if we don't know the underlying distribution from which the data is drawn.\n",
    "2. The distribution is very narrowly centered about the true mean, with a small standard deviation. This means that we can expect our estimates of $\\bar{X}$ to be very similar if we run the experiment multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 500\n",
    "n_samples = 10000\n",
    "samples = np.zeros(n_samples)\n",
    "for j in range(n_samples):\n",
    "    samples[j] = np.random.binomial(n, 0.8)/n\n",
    "    \n",
    "plt.hist(samples, bins=20);\n",
    "plt.xlabel('y/n')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the second point above, the full statement of the CLT actually explains this observation. According to the CLT, the distribution of means when $n$ is large is given by $\\bar{X} \\sim \\mathcal{N} (\\mu, \\sigma^2/n)$, where $\\mu$ and $\\sigma^2$ are the mean and variance (squared standard deviation) of the distribution over $X$. For a binary variable, these are $\\mu = p$ and $\\sigma^2 = p(1-p)$. Notice that the variance in the distribution gets smaller as $n$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "\n",
    "Compute `samples` as above using the following values of `n`: 10, 100, 1000, and 10000. In each case, calculate and print the standard deviation of the values in `samples`, as well as the theoretical value for the standard deviation given above. The values should match to within a few percent. As before, use `n_samples` = 10000 and $p=0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum likelihood\n",
    "\n",
    "When fitting a statistical model (for example, a normal distribution to describe a collection of data points, or a linear fit to describe the relation between two sets of data) to data, we want to optimize the parameters of the fit. Maximum likelihood (ML) gives us a way to do this. According to ML, we should write think of our data $x$ as coming from a probability distribution $p(x|\\theta)$ that depends on parameter $\\theta$ (which could be, for example, the mean $\\mu$ or variance $\\sigma^2$ in a normal distribution). The best value of $\\theta$ is then the one that maximizes this probability distribution. In practice, we actually maximize $\\ln(p(x|\\theta))$ rather than $p(x|\\theta)$ itself. We do this because it is easier mathematically, and, since the logarithm always increases when its argument increases, we can be sure that the $\\theta$ that maximizes $p(x|\\theta)$ will also maximize $\\ln(p(x|\\theta))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML for the normal distribution\n",
    "\n",
    "In class, we noted that the probability of observing a data point $x_i$ from a normal distribution is \n",
    "$$\n",
    "p(x_i | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2\\sigma^2}(x_i - \\mu)^2},\n",
    "$$\n",
    "where $\\mu$ and $\\sigma$ are the parameters that were referred to as $\\theta$ above. If we have lots of independent data points, then the probability of observing all of them is just the product of the individual probabilities:\n",
    "$$\n",
    "p(x_1, x_2, \\ldots, x_n | \\mu, \\sigma) = \\prod_{i=1}^n p(x_i | \\mu, \\sigma).\n",
    "$$\n",
    "Then, because the logarithm of a product is equal to the sum of logarithms, the above expression leads to the \"log likelihood\":\n",
    "$$\n",
    "\\ln p(x_1, x_2, \\ldots, x_n | \\mu, \\sigma) = \\sum_{i=1}^n \\ln p(x_i | \\mu, \\sigma)\n",
    "= \\sum_{i=1}^n \\left[ - \\ln(\\sqrt{2\\pi}\\sigma) - \\frac{(x_i - \\mu)^2}{2 \\sigma^2}\\right].\n",
    "$$\n",
    "In class we showed that, by setting the derivative of this expression with respect to $\\mu$ equal to zero, we could find the optimal value of $\\mu$, which we called $\\hat{\\mu}$:\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i,\n",
    "$$\n",
    "which just says that the normal distribution should be centered at the mean of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5 (calculation)**\n",
    "\n",
    "Take the derivative of the above log likelihood with respect to $\\sigma$ and set this derivative equal to zero in order to find the optimal value $\\hat{\\sigma}$. Before you begin, make a guess (just to yourself, no need to write it down) about what you expect the answer to be, then see whether your guess is right. As usual, you can either type your solution using LaTex notation below or submit it as a PDF file. Please show your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence intervals\n",
    "\n",
    "Above we considered a coin flipping experiment with $n=10$ flips of a biased coin. If we count an occurrence of heads in flip $i$ as $X_i=1$ and tails as $X_i=0$, then the outcome of our experiment is a random variable $Y = \\sum_{i=1}^n X_i$, which is described by a binomial distribution: $Y \\sim \\mathcal{B}(n,p)$, where $p$ is the true probability of a given flip coming up heads. \n",
    "\n",
    "Our goal now is to estimate $p$ and to quantify our confidence in that estimate. In class we showed that the maximum likelihood estimate of $p$ for a binomial distribution given data $Y$ is $\\hat{p} = Y/n$. The standard error then comes from calculating the variance of this quantity, which we showed in class is $Var(Y/n) = p(1-p)/n$. The standard error is given by the square root of this variance, but with $p$ replaced by our estimate $\\hat{p}$ since we don't know the true value of $p$, giving $\\text{SE}(\\hat{p}) = \\sqrt{\\hat{p}(1-\\hat{p})/n}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range $[\\hat{p}-\\text{SE}(\\hat{p}), \\hat{p}+\\text{SE}(\\hat{p})]$ can be referred to as a **confidence interval**. If we could repeat our experiment many times and calculate the confidence interval for each experiment, the interval would include, or \"cover\", the true value of $p$ most of the time (we'll show exactly how often below). (Note that, as we discussed in class, this is slightly different from saying that the true value of $p$ lies inside our confidence interval with a certain probability--a common misinterpretation!) We can also note that the confidence interval becomes smaller as $n$ grows larger, which tells us that we get a better estimate of $p$ as we do more coin flips. \n",
    "\n",
    "In addition, we can note that we could define a larger confidence interval, for example $[\\hat{p}-2\\text{SE}(\\hat{p}), \\hat{p}+2\\text{SE}(\\hat{p})]$, and we would be more confident that the data lies within this larger interval than within the smaller interval. If, for example, we have a large number of data points $n$ so that the CLT applies, then it is possible to show that the interval $[\\hat{p}-2\\text{SE}(\\hat{p}), \\hat{p}+2\\text{SE}(\\hat{p})]$ is expected to cover the true value of $p$ 95% of the time. Hence, we refer to this as the \"95% confidence interval\".\n",
    "\n",
    "It would be difficult to illustrate all of this with real experimental data since it would require having data from a large number of repetitions of the entire experiment. Since we are doing a simulated experiment, though, we can repeat it as many times as we like! In the following exercise, we'll use this ability to illustrate the above statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**\n",
    "\n",
    "1. Use a for loop to repeat the coin flipping experiment with `n=20` and `p=0.8` 10000 times, each time computing $\\hat{p}$ and $\\text{SE}(\\hat{p})$. Print the fraction of the experiments in which the true value of $p$ lies within the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Earlier in this notebook we showed how the binomial distribution becomes a normal distribution when $n$ is large due to the CLT. Repeat the previous exercise, but now with `n=500`. Also change the confidence interval to $[\\hat{p}-2\\text{SE}(\\hat{p}), \\hat{p}+2\\text{SE}(\\hat{p})]$. Show that the fraction of experiments in which the confidence interval covers the true value of $p$ is very close to 0.95. This illustrates the \"95% rule\", which states that, for data that comes from a normal distribution, 95% of the probability is within two standard deviations of the mean. Hence, for a given experiment of $n$ coin flips, we can refer to the range $[\\hat{p}-2\\text{SE}(\\hat{p}), \\hat{p}+2\\text{SE}(\\hat{p})]$ as the \"95% confidence interval\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**\n",
    "\n",
    "In class and in the textbook an example was discussed in which a patient with brain damage correctly guessed in 14/17 cases whether a picture of a house was burning, even though she claimed not to be able to see the difference between a burning and non-burning house. Was the patient guessing randomly?\n",
    "\n",
    "1. If we model the patient's behavior as a binomial distribution where $p$ is the probability of the patient making a correct guess, compute the interval $[\\hat{p}-\\text{SE}(\\hat{p}), \\hat{p}+\\text{SE}(\\hat{p})]$ and print the estimate as $\\hat{p} \\pm \\text{SE}(\\hat{p})$. Does this range include $p=0.5$? If not, this provides evidence that the patient was not randomly guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Simulate 10000 binomial experiments with $n=17$ and $p=0.5$ and compute $\\hat{p}$ for each. In what fraction of the experiments is $\\hat{p}$ greater than or equal to the value that you computed above for the patient? This is an example of a \"p value\" (sorry, this p is different from the other one--p means lots of different things in statistics!). If the p value is small (conventionally, smaller than 0.05 is considered to be significant), this tells us that the observed data is likely to be inconsistent with the statistical model, allowing us to \"reject the null hypothesis\". The idea of a p value is very important, since it is the typical way that experimental conclusions are stated. We'll learn more about them in the next homework assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a value that is well below 0.05, which tells us that it is very unlikely that the patient was randomly guessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
